{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1XOQ04ZL8Rz"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/69cbc0371777fba5d251a564e2f8a8f38d1bf43f/68747470733a2f2f6a756e79616e7a2e6769746875622e696f2f4379636c6547414e2f696d616765732f7465617365725f686967685f7265732e6a7067\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsqQ3iSpVL7i"
   },
   "source": [
    "# Photo -> Map\n",
    "\n",
    "\n",
    "Previously, we used neural networks for **sparse** predictions: large input (image) -> small output (vector with 10 elements, e.g. CIFAR10 classes). Today we'll use deep learning to make **dense** predictions (large input (image) -> large output (image)) for **image-to-image translation problem**. *Image-to-image translation* is a wide class of problems, where input is image and output is image too (e.g. satellite photo -> map, image stylization, [sketch -> cat portrait](https://affinelayer.com/pixsrv/),  etc...). There many good models for dense predictions, but we'll use **UNet** as the best choice in terms of simplicity-quality ratio.\n",
    "\n",
    "But before we start, let's look at our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwBjoCGoVL7f"
   },
   "source": [
    "<img src=\"https://github.com/karfly/learning-deep-learning/blob/master/04_dense/static/photo2map.jpg?raw=true\" width=700 align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WV_63asxTuUB"
   },
   "source": [
    "## Task 1. Dataset\n",
    "We'll create a dataset of pairs **satellite photo - map** (example is above). To download dataset, uncomment and execute the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ukUPLy5TuUC"
   },
   "outputs": [],
   "source": [
    "! wget https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/maps.tar.gz\n",
    "! tar -xzvf maps.tar.gz\n",
    "! mkdir maps/train/0 && mv maps/train/*.jpg maps/train/0\n",
    "! mkdir maps/val/0 && mv maps/val/*.jpg maps/val/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYGylIjITuUJ"
   },
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPY2bklBTuUK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.notebook import tqdm    # run this in Colab\n",
    "from tqdm import tqdm               # or this in Jupyter instead\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4FBVTIKTuUM"
   },
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HVgUuibTuUN"
   },
   "outputs": [],
   "source": [
    "experiment_title = \"unet\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 4\n",
    "image_size = 256\n",
    "\n",
    "data_dir = \"./maps\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0CNmQPdTuUQ"
   },
   "source": [
    "After downloading and unpacking you'll find directory `maps` with 2 subdirectories: `train` and `val`. Each image is a pair (photo - map), so we'll have to **crop image to obtain input and target**. Let's use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder) dataloader: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dAArjkWTuUQ"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvSpaj_ITuUT"
   },
   "source": [
    "Draw sample from dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bmfsif_3Z_JO"
   },
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taJpEKtCTuUa"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    img = img.cpu().numpy()\n",
    "    \n",
    "    img_reshaped = np.transpose(img, (1, 2, 0))\n",
    "\n",
    "    # below we use einops.rearrange. You can read about this neat library here: https://github.com/arogozhnikov/einops/blob/master/docs/1-einops-basics.ipynb\n",
    "    # img_reshaped = rearrange(img, 'channels height width -> height width channels')\n",
    "    \n",
    "    plt.imshow(img_reshaped)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WU6I0pSvTuUc"
   },
   "outputs": [],
   "source": [
    "(image, _) = train_dataset[0]\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjlC-yC6TuUf"
   },
   "source": [
    "As we can see input and target are in the same image. Let's write wrapper of ImageFolder to return what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nmVEdPaTuUg"
   },
   "outputs": [],
   "source": [
    "class PhotoMapDataset(torchvision.datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, _ = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        photo_image, map_image = # YOUR CODE HERE\n",
    "        \n",
    "        return photo_image, map_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qA4_Ok8TuUi"
   },
   "source": [
    "So now we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grY72UkYTuUj"
   },
   "outputs": [],
   "source": [
    "train_dataset = PhotoMapDataset(root=os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhY4JKvATuUn"
   },
   "outputs": [],
   "source": [
    "photo_image, map_image = train_dataset[0]\n",
    "imshow(photo_image)\n",
    "imshow(map_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ5GNMjcTuUr"
   },
   "source": [
    "Okay, we're don with data. Let's move to defining model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqxpvoCMTuUs"
   },
   "source": [
    "## Task 2. U-Net\n",
    "UNet is a very popular fully-convolutional architecture. Below you can find its structure (for more details refer to [original paper](https://arxiv.org/abs/1505.04597)):\n",
    "\n",
    "<!-- <img src=\"https://github.com/karfly/learning-deep-learning/blob/master/04_dense/static/unet.png?raw=true\" width=800 align=\"center\"/> -->\n",
    "\n",
    "<img src=\"https://i.imgur.com/g9niT97.png\" width=800 align=\"center\"/>\n",
    "\n",
    "Let's build U-Net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNjNVQgzfIky"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0f_2MrGTuUu"
   },
   "outputs": [],
   "source": [
    "# UNetDownBlock: Conv + ReLU + Conv + ReLU + MaxPool\n",
    "\n",
    "class UnetDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: a tensor of shape (batch_size, in_channels, layer_height, layer_width)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        return out, out_before_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5nVMvL5TuUz"
   },
   "outputs": [],
   "source": [
    "# UNetUpBlock: upsampling + concat + Conv + ReLU\n",
    "\n",
    "class UnetUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x, x_bridge):\n",
    "        # x: a tensor of shape (batch_size, in_channels, layer_height // 2, layer_width // 2)\n",
    "        # x_bridge: a tensor of shape (batch_size, in_channels, layer_height, layer_width)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_kqF3LUTuU1"
   },
   "outputs": [],
   "source": [
    "### Let's make a UNet with 5 levels and 64, 128, 256, 256, 256 channels\n",
    "### (n_base_channels=64)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, n_base_channels=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYlpUensTuU4"
   },
   "outputs": [],
   "source": [
    "model = Unet(n_base_channels=64).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GguKJiG8TuU8"
   },
   "source": [
    "## Train-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlEsbUaHTuU9"
   },
   "source": [
    "Optimization setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8lciLZwTuU9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU3dFS0fTuVA"
   },
   "source": [
    "Setting up SummaryWriter for TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hErsI9F9TuVB"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "experiment_name = \"{}@{}\".format(experiment_title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "print('experiment_name:', experiment_name)\n",
    "writer = SummaryWriter(log_dir=os.path.join(\"./tb\", experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YVkrmH4QofhP"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCJ0hPc8og4d"
   },
   "outputs": [],
   "source": [
    "logs_dir = os.path.join(\"./tb\", experiment_name)\n",
    "%tensorboard --logdir {logs_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DLDD4ay8TuVD"
   },
   "source": [
    "Train-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlqinVxvTuVD"
   },
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    n_iters = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # unpack batch\n",
    "        photo_image_batch, map_image_batch = batch\n",
    "        photo_image_batch, map_image_batch = photo_image_batch.to(device), map_image_batch.to(device)\n",
    "        \n",
    "        # forward\n",
    "        map_image_pred_batch = model(photo_image_batch)\n",
    "\n",
    "        loss = criterion(map_image_pred_batch, map_image_batch)\n",
    "        \n",
    "        # optimize\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # dump statistics\n",
    "        writer.add_scalar(\"train/loss\", loss.item(), global_step=epoch * len(train_dataloader) + n_iters)\n",
    "        \n",
    "        if n_iters % 50 == 0:\n",
    "            writer.add_image('train/photo_image', torchvision.utils.make_grid(photo_image_batch), epoch * len(train_dataloader) + n_iters)\n",
    "            writer.add_image('train/map_image_pred', torchvision.utils.make_grid(map_image_pred_batch), epoch * len(train_dataloader) + n_iters)\n",
    "            writer.add_image('train/map_image_gt', torchvision.utils.make_grid(map_image_batch), epoch * len(train_dataloader) + n_iters)\n",
    "        \n",
    "        n_iters += 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    print(\"Epoch {} done.\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIjDfWz2TuVG"
   },
   "source": [
    "### FYI\n",
    "\n",
    "Fully-Convolutional Networks similar to the one trained today can perform extremely well for this task when segmentation losses are used, not MSELoss. If interested, check out this: https://www.youtube.com/watch?v=yUyaYL_x4kQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seminar_photo2map.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
